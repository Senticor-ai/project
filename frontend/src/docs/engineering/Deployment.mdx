import { Meta } from "@storybook/addon-docs/blocks";

<Meta title="Engineering/Deployment" />

# Deployment

Senticor Project uses a **GitOps** deployment model. Code changes on `main` trigger an automated pipeline that builds container images, pushes them to a private registry, and lets a cluster-side reconciler apply the updated Kubernetes manifests. No manual `kubectl apply` is needed.

---

## Validation Pipeline

The pipeline validates every change before it reaches the cluster:

```
Push to main
  │
  ├─ Build ──────── Kaniko validates Docker builds for frontend, backend,
  │                  and storybook images (no push in build stage)
  │
  ├─ Push ───────── images pushed to private Harbor registry
  │                  using a CI robot account (credentials in CI/CD variables)
  │
  ├─ SAST ───────── GitLab static application security testing
  │
  └─ Reconcile ──── Flux CD detects changes in infra/k8s/overlays/production/
                     and applies Kustomize manifests to the cluster (~5 min)
```

### What triggers a deployment

| Change                                                | Effect                                   |
| ----------------------------------------------------- | ---------------------------------------- |
| Application code (frontend/ or backend/)              | CI rebuilds images → new pods roll out   |
| Kubernetes manifests (infra/k8s/overlays/production/) | Flux applies updated resources           |
| Both in same commit                                   | Images built first, then Flux reconciles |

### CI/CD Configuration

The pipeline is defined in `.gitlab-ci.yml` and uses Kaniko jobs per image Dockerfile.
Two masked CI/CD variables must be configured in GitLab Senticor Project settings:

- `HARBOR_USER` — robot account for the private registry
- `HARBOR_PASSWORD` — robot account secret

For this Senticor Project, CI uses Kaniko (no Docker daemon). Harbor uses an internal CA certificate that is not publicly trusted, so Kaniko runs with `--insecure --skip-tls-verify` when pushing images.
The CI job writes `/kaniko/.docker/config.json` from those two variables instead of calling `docker login`.
On `main`, images are pushed with the `CI_COMMIT_SHORT_SHA` tag and the production
Kustomization is updated to that same short SHA.

These are the only secrets stored in GitLab. All application secrets live in the cluster, managed by ops.

---

## Container Structure

The Senticor Project has three Dockerfiles, all built from the monorepo root context:

### Frontend (`frontend/Dockerfile`)

Multi-stage build:

1. **Build stage** — `node:24-alpine`, runs `npm ci` and `npx vite build`
2. **Runtime stage** — `nginx:1.27-alpine`, serves the built static files

Nginx proxies `/api/*` requests to the backend service and serves the SPA with `try_files` fallback. Exposes port **80**.

### Backend (`backend/Dockerfile`)

Single-stage build:

1. `python:3.12-slim` base image
2. Uses `uv` for dependency management (`uv sync --frozen --no-dev`)
3. Runs `uvicorn app.main:app` on port **8000**

### Storybook (`frontend/Dockerfile.storybook`)

Multi-stage build:

1. **Build stage** — `node:24-alpine`, runs `storybook build` with `STORYBOOK_BASE`
2. **Runtime stage** — `nginx:1.27-alpine`, serves static Storybook assets

---

## Kubernetes Architecture

All application resources are declared in `infra/k8s/` using a Kustomize base/overlay structure. The production overlay lives in `infra/k8s/overlays/production/`. Flux injects the target namespace automatically — production manifests must not set `namespace:`.

### Resources managed by the team (in `infra/k8s/`)

| Resource      | Type                  | Notes                                                      |
| ------------- | --------------------- | ---------------------------------------------------------- |
| frontend      | Deployment + Service  | Nginx serving SPA, proxying `/api/*` to backend            |
| backend       | Deployment + Service  | FastAPI with init containers for DB readiness + migration  |
| worker        | Deployment            | Projection outbox worker (indexing/import/email sync)      |
| push-worker   | Deployment            | Web Push outbox delivery worker                            |
| watch-worker  | Deployment            | Gmail Pub/Sub watch worker (real-time sync trigger)        |
| postgres      | StatefulSet + Service | PostgreSQL 16 with 10Gi persistent volume                  |
| backend-files | PVC (5Gi)             | File storage mounted at `/data/storage`                    |
| app-config    | ConfigMap             | Non-sensitive configuration (CORS, logging, feature flags) |

### Resources managed by ops (not in version control)

| Resource  | Purpose                                                                   |
| --------- | ------------------------------------------------------------------------- |
| Namespace | Pre-created by ops                                                        |
| Secret    | Application secrets (see [secrets reference](#application-secrets) below) |
| Ingress   | Routes external traffic to the frontend service                           |

This separation ensures secrets never appear in version control.

### Application Secrets

The `app-secrets` Secret must include these keys:

| Key                   | Description                     | How to generate                                                                             |
| --------------------- | ------------------------------- | ------------------------------------------------------------------------------------------- |
| `POSTGRES_PASSWORD`   | PostgreSQL password             | &mdash;                                                                                     |
| `JWT_SECRET`          | Internal token signing          | `python -c "import secrets; print(secrets.token_urlsafe(32))"`                              |
| `GMAIL_CLIENT_ID`     | Google OAuth client ID          | [Cloud Console](https://console.cloud.google.com/apis/credentials)                          |
| `GMAIL_CLIENT_SECRET` | Google OAuth client secret      | Same as above                                                                               |
| `GMAIL_STATE_SECRET`  | OAuth CSRF state signing        | `python -c "import secrets; print(secrets.token_urlsafe(32))"`                              |
| `ENCRYPTION_KEY`      | Fernet key for token encryption | `python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"` |

See [Email Integration](?path=/docs/engineering-email-integration--docs) for the full Gmail OAuth setup guide.

### Database Initialization

The backend Deployment uses init containers instead of a standalone Job (Jobs are not idempotent in GitOps — Flux would recreate them every cycle):

1. **wait-for-postgres** — polls `pg_isready` until the database accepts connections
2. **db-init** — runs `uv run python -m app.db_init`, which executes `alembic upgrade head`

This runs on every pod start, which is safe because Alembic applies only pending revisions.

### Resource Budget

The cluster namespace has a resource quota. Current allocation:

| Component    | CPU request | Memory request | CPU limit | Memory limit |
| ------------ | ----------- | -------------- | --------- | ------------ |
| Frontend     | 50m         | 128Mi          | 200m      | 256Mi        |
| Backend      | 200m        | 512Mi          | 1000m     | 2Gi          |
| Worker       | 100m        | 256Mi          | 500m      | 1Gi          |
| Push Worker  | 50m         | 128Mi          | 250m      | 512Mi        |
| Watch Worker | 50m         | 128Mi          | 250m      | 512Mi        |
| PostgreSQL   | 200m        | 256Mi          | 500m      | 1Gi          |
| **Total**    | **650m**    | **1408Mi**     | **2700m** | **5.25Gi**   |

Contact ops if the quota needs to be increased.

---

## Observability

The backend exposes three observability signals: **structured logs**, **Prometheus metrics**, and **OpenTelemetry traces**. All three are collected automatically in the cluster.

### Logs

Structured JSON logs via `structlog`. Every log entry includes `trace_id` and `span_id` when OTEL tracing is active, enabling Loki ↔ Tempo correlation in Grafana. Logs are collected automatically into Loki.

| Field        | Source                                           |
| ------------ | ------------------------------------------------ |
| `request_id` | `X-Request-ID` header (or server-generated UUID) |
| `trace_id`   | OTEL span context (32-hex)                       |
| `span_id`    | OTEL span context (16-hex)                       |
| `user_id`    | Session auth                                     |

Set `LOG_FORMAT=console` in the ConfigMap for human-readable output during local development.

### Prometheus Metrics

All four worker/API deployments expose `/metrics` in Prometheus format with scrape annotations already configured:

| Deployment        | Port | Endpoint   |
| ----------------- | ---- | ---------- |
| Backend           | 8000 | `/metrics` |
| Projection Worker | 9090 | `/metrics` |
| Push Worker       | 9091 | `/metrics` |
| Watch Worker      | 9092 | `/metrics` |

**HTTP server metrics** (backend):

| Metric                                 | Type      | Labels                      |
| -------------------------------------- | --------- | --------------------------- |
| `http_server_requests_total`           | Counter   | method, route, status_class |
| `http_server_request_duration_seconds` | Histogram | method, route, status_class |
| `http_server_in_flight_requests`       | Gauge     | —                           |

**Queue health metrics** (backend, refreshed on scrape):

| Metric                         | Type  | Labels        |
| ------------------------------ | ----- | ------------- |
| `app_queue_depth`              | Gauge | queue         |
| `app_queue_oldest_age_seconds` | Gauge | queue         |
| `app_queue_jobs_by_status`     | Gauge | queue, status |
| `app_queue_dead_lettered`      | Gauge | queue         |

Queues tracked: `outbox_events`, `push_outbox`, `import_jobs`, `search_index_jobs`.

**Business metrics** (backend + workers):

| Metric                        | Type    | Labels                   |
| ----------------------------- | ------- | ------------------------ |
| `app_items_created_total`     | Counter | bucket                   |
| `app_items_updated_total`     | Counter | —                        |
| `app_items_archived_total`    | Counter | —                        |
| `app_imports_completed_total` | Counter | source (nirvana, native) |
| `app_imports_failed_total`    | Counter | source                   |

**Worker health metrics** (workers):

| Metric                          | Type      | Labels |
| ------------------------------- | --------- | ------ |
| `worker_batches_total`          | Counter   | worker |
| `worker_events_total`           | Counter   | worker |
| `worker_errors_total`           | Counter   | worker |
| `worker_batch_duration_seconds` | Histogram | worker |
| `worker_up`                     | Gauge     | worker |

### OpenTelemetry Tracing

OTEL tracing is initialised at app startup via `app/tracing.py`. The SDK auto-reads the standard environment variables from the ConfigMap:

| Variable                      | Value (production)                                            |
| ----------------------------- | ------------------------------------------------------------- |
| `OTEL_SERVICE_NAME`           | `project-backend`                                             |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | `http://alloy.observability.svc.cluster.local:4318`           |
| `OTEL_EXPORTER_OTLP_PROTOCOL` | `http/protobuf`                                               |
| `OTEL_RESOURCE_ATTRIBUTES`    | `service.namespace=project,deployment.environment=production` |

FastAPI routes are auto-instrumented — every inbound request creates a span with method, route, and status. Traces are exported via `BatchSpanProcessor` to the Alloy collector, which forwards to Tempo.

**Local development:** When `OTEL_EXPORTER_OTLP_ENDPOINT` is not set, tracing is silently disabled. The app logs `tracing.disabled` and runs without any OTEL overhead.

---

## Local Development

Start the full dev stack from the repo root:

```bash
npm run dev   # backend + worker + frontend + storybook
```

This uses `concurrently` to run all four processes with color-coded output. Ctrl+C stops everything. Individual startup scripts live in `scripts/start_*.sh`.

Infrastructure services (PostgreSQL, Meilisearch) still need to be started separately via `infra/docker-compose.yml`.

For local **Kubernetes** development, use the local overlay (`infra/k8s/overlays/local/`) with Rancher Desktop. It shares the same base manifests as production but uses local image builds, dev-friendly ConfigMap values, and includes namespace/secret/ingress resources.

---

## Related Documentation

- [Architecture](?path=/docs/engineering-architecture--docs) — Technology stack, dual-store model, and architectural principles
- [Email Integration](?path=/docs/engineering-email-integration--docs) — Gmail OAuth setup, environment variables, troubleshooting
